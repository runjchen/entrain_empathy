# -*- coding: utf-8 -*-
"""entrain-encoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1stERozOzEwgFQEaf1IQAbKwChEumYH6I

# Dependencies
"""

# !pip install transformers --upgrade --user
!pip install transformers
# !conda install pytorch torchvision -c pytorch --yes
# restart run time after installing

# upload the entrain-data folder
!mkdir "entrain-data"
!mv *.pkl entrain-data

import os
import numpy as np
import pandas as pd
from collections import defaultdict
import pickle

# import parselmouth
# from parselmouth.praat import call

import matplotlib.pyplot as plt
from matplotlib.pyplot import errorbar, boxplot
import seaborn as sns

from scipy import stats
import random

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix

from tqdm import tqdm

import nltk
from nltk.tokenize import word_tokenize
import re

import torch
import torch.nn as nn
import torch.nn.functional as F

from transformers import BertTokenizer, BertModel

import time
import datetime

def format_time(elapsed):
  elapsed_rounded = int(round((elapsed)))
  return str(datetime.timedelta(seconds=elapsed_rounded))

from torch import cuda
device = 'cuda' if cuda.is_available() else 'cpu'
print('Device count:', cuda.device_count())
print('Using device:', device)

#Additional Info when using cuda
if device == 'cuda':
  print(torch.cuda.get_device_name(0))
  print(torch.cuda.get_device_properties(0))
  print('Memory Usage:')
  print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')
  print('Cached: ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')

SPEECH_FEATURES = ['min_pitch', 'max_pitch','mean_pitch', 'sd_pitch',
            		'min_intensity', 'max_intensity', 
            		'mean_intensity', 'sd_intensity',
            		'jitter', 'shimmer', 'hnr', 'speaking_rate']

"""# load the entrain data"""

entrain_data = []
for f in os.listdir('entrain-data'):
  with open(os.path.join('entrain-data', f), "rb") as input_file:
      entrain_data.extend(pickle.load(input_file))
print(len(entrain_data))
print(entrain_data[0])

"""## tokenize the data"""

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
# print(tokenizer.encode("i am happy. I am sad. [SEP] pearl is good.", add_special_tokens=True))
# print(tokenizer.encode("i am happy I am sad.", add_special_tokens=True))
# print(tokenizer.encode(".", add_special_tokens=True))

# # get max length of sentence token for padding
# max_len = 0
# count = 0

# for context, sentence, diff in entrain_data:
#   input_ids = tokenizer.encode(' [SEP] '.join([context,sentence]), add_special_tokens=True) #add `[CLS]` and `[SEP]` tokens.
#   max_len = max(max_len, len(input_ids))
max_len = 312
print('Max sentence length: ', max_len)

input_ids = []
attention_masks = []
diffs = []
# for sent in sentences:
for context, sentence, diff in entrain_data:
  if np.isnan(np.sum(diff)): #remove nan in speech featurs, 138 counts
    pass

  else:
    encoded_dict = tokenizer.encode_plus(
                        ' [SEP] '.join([context,sentence]),
                        add_special_tokens = True, 
                        max_length = max_len,
                        pad_to_max_length = True,
                        return_attention_mask = True,
                        return_tensors = 'pt',
                   )
       
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])
    diffs.append(diff)

# Convert the lists into tensors.
input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(diffs, dtype=torch.float)
print(input_ids.shape, attention_masks.shape, labels.shape)

# Print last sentence, now as a list of IDs.
print('Original: ', sentence)
print('Token IDs:', input_ids[-1])

"""## split train/val dataset"""

from torch.utils.data import TensorDataset, random_split

dataset = TensorDataset(input_ids, attention_masks, labels)

# Create a 90-10 train-validation split.
train_size = int(0.9 * len(dataset))
val_size = len(dataset) - train_size

train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

print('{:>5,} training samples'.format(train_size))
print('{:>5,} validation samples'.format(val_size))

from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
batch_size = 32

train_dataloader = DataLoader(
            train_dataset,
            sampler = RandomSampler(train_dataset),
            batch_size = batch_size
        )

validation_dataloader = DataLoader(
            val_dataset,
            sampler = SequentialSampler(val_dataset),
            batch_size = batch_size
        )

"""# Model"""

class BERT_Arch(nn.Module):
  def __init__(self, bert, output_size=12):
    super(BERT_Arch, self).__init__()
    self.bert = bert
    # self.fc = nn.Linear(768, output_size)
    self.fc1 = nn.Linear(768,128)
    self.fc2 = nn.Linear(128, output_size)
    
    #activation functions
    self.dropout1 = nn.Dropout(0.1)
    # self.dropout2 = nn.Dropout(0.1)
    self.relu1 = nn.ReLU()
    # self.relu2 = nn.ReLU()
    # self.softmax = nn.LogSoftmax(dim=1)

  def forward(self, sent_id, mask):
    bert_out = self.bert(sent_id, attention_mask=mask).pooler_output #(batch_size, hidden_size)
    x = self.fc1(bert_out)
    x = self.relu1(x)
    x = self.dropout1(x)

    x = self.fc2(x)
    # x = self.relu2(x)
    # x = self.dropout2(x)

    # x = self.fc3(x)
    # x = self.softmax(x)
    return x

from transformers import BertModel, AdamW, BertConfig
bert = BertModel.from_pretrained('bert-base-uncased')
model = BERT_Arch(bert, output_size=12)
criterion = nn.MSELoss()
optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)

from transformers import get_linear_schedule_with_warmup

# Number of training epochs. The BERT authors recommend between 2 and 4. 
# We chose to run for 4, but we'll see later that this may be over-fitting the
# training data.
epochs = 5

# Total number of training steps is [number of batches] x [number of epochs]. 
# (Note that this is not the same as the number of training samples).
total_steps = len(train_dataloader) * epochs

# Create the learning rate scheduler.
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = 0,
                                            num_training_steps = total_steps)

# push the model to GPU
model = model.to(device)
print(model)

"""# Training"""

# This training code is based on the `run_glue.py` script here:
# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128

# Set the seed value all over the place to make this reproducible.

seed_val = 42

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

training_stats = []

total_t0 = time.time() # Measure the total training time for the whole run.

for epoch_i in range(0, epochs):
    print("")
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    
    ## =================== Training =================== ##
    t0 = time.time()
    total_train_loss = 0
    
    model.train() # turn on dropout layers
    loop = tqdm(enumerate(train_dataloader), total=len(train_dataloader))
    
    for step, batch in loop:
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)
       
        optimizer.zero_grad() 

        ### forward pass ###
        b_train_pred = model(b_input_ids, mask=b_input_mask)
        loss = criterion(b_train_pred, b_labels)
        total_train_loss += loss.item() * b_input_ids.shape[0]
        
        ### backward pass ###
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #avoid gradient explode
        optimizer.step()
        scheduler.step()

        ### update progress bar ###
        loop.set_postfix(loss=loss.item())
    
    avg_train_loss = total_train_loss / train_dataloader.dataset.__len__()        
    training_time = format_time(time.time() - t0)
    
    print("  Average training loss: {0:.2f}".format(avg_train_loss))
    print("  Training epcoh took: {:}".format(training_time)) 
        
    ## =================== Validation =================== ##
    t0 = time.time()

    model.eval()

    # Tracking variables 
    total_eval_loss = 0

    # Evaluate data for one epoch
    for batch in tqdm(validation_dataloader):
        
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)
        
        with torch.no_grad():        
            b_train_pred = model(b_input_ids, mask=b_input_mask)
            loss = criterion(b_train_pred, b_labels)
            
        total_eval_loss += loss.item()*b_input_ids.shape[0]
    
    avg_val_loss = total_eval_loss / validation_dataloader.dataset.__len__()
    validation_time = format_time(time.time() - t0)

    print("  Validation Loss: {0:.2f}".format(avg_val_loss))
    print("  Validation took: {:}".format(validation_time))
    
    # Record all statistics from this epoch.
    training_stats.append(
        {
            'epoch': epoch_i + 1,
            'Training Loss': avg_train_loss,
            'Valid. Loss': avg_val_loss,
            'Training Time': training_time,
            'Validation Time': validation_time,
        }
    )
    
    #save model
    torch.save({
            'epoch': epoch_i+1,
            'model_state_dict': model.state_dict(),
            'bert_state_dict':model.bert.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict':scheduler.state_dict(),
            'avg_train_loss': avg_train_loss,
            }, f'./checkpoint_{epoch_i+1}.pt')

print("Training complete!")

print("Total training took {:} (h:mm:ss)".format(format_time(time.time()-total_t0)))

total_t0 = time.time() # Measure the total training time for the whole run.

for epoch_i in range(0+5, epochs+5):
    print("")
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    
    ## =================== Training =================== ##
    t0 = time.time()
    total_train_loss = 0
    
    model.train() # turn on dropout layers
    loop = tqdm(enumerate(train_dataloader), total=len(train_dataloader))
    
    for step, batch in loop:
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)
       
        optimizer.zero_grad() 

        ### forward pass ###
        b_train_pred = model(b_input_ids, mask=b_input_mask)
        loss = criterion(b_train_pred, b_labels)
        total_train_loss += loss.item() * b_input_ids.shape[0]
        
        ### backward pass ###
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #avoid gradient explode
        optimizer.step()
        scheduler.step()

        ### update progress bar ###
        loop.set_postfix(loss=loss.item())
    
    avg_train_loss = total_train_loss / train_dataloader.dataset.__len__()        
    training_time = format_time(time.time() - t0)
    
    print("  Average training loss: {0:.2f}".format(avg_train_loss))
    print("  Training epcoh took: {:}".format(training_time)) 
        
    ## =================== Validation =================== ##
    t0 = time.time()

    model.eval()

    # Tracking variables 
    total_eval_loss = 0

    # Evaluate data for one epoch
    for batch in tqdm(validation_dataloader):
        
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)
        
        with torch.no_grad():        
            b_train_pred = model(b_input_ids, mask=b_input_mask)
            loss = criterion(b_train_pred, b_labels)
            
        total_eval_loss += loss.item()*b_input_ids.shape[0]
    
    avg_val_loss = total_eval_loss / validation_dataloader.dataset.__len__()
    validation_time = format_time(time.time() - t0)

    print("  Validation Loss: {0:.2f}".format(avg_val_loss))
    print("  Validation took: {:}".format(validation_time))
    
    # Record all statistics from this epoch.
    training_stats.append(
        {
            'epoch': epoch_i + 1,
            'Training Loss': avg_train_loss,
            'Valid. Loss': avg_val_loss,
            'Training Time': training_time,
            'Validation Time': validation_time,
        }
    )
    
    #save model
    torch.save({
            'epoch': epoch_i+1,
            'model_state_dict': model.state_dict(),
            'bert_state_dict':model.bert.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict':scheduler.state_dict(),
            'avg_train_loss': avg_train_loss,
            }, f'./checkpoint_{epoch_i+1}.pt')

print("Training complete!")

print("Total training took {:} (h:mm:ss)".format(format_time(time.time()-total_t0)))

"""# Train/Val loss curve"""

training_stats

stats = pd.DataFrame(training_stats)
stats

sns.set(style='darkgrid')
sns.set(font_scale=1.5)
plt.rcParams["figure.figsize"] = (12,6)
plt.plot(stats['Training Loss'], 'b-o', label="Training")
plt.plot(stats['Valid. Loss'], 'g-o', label="Validation")
# Label the plot.
plt.title("Training & Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.xticks([i for i in range(0, 10)], #locs
            [i for i in range(1, 12)], #labels
)
plt.show()

"""We save the model at checkpoint 5."""

!ls -Shlr

# from google.colab import files
# files.download('checkpoint_5.pt')

# !zip -r checkpoint_5.zip checkpoint_5.pt

# files.download('checkpoint_5.zip')

# !cp "checkpoint_5.zip" "drive/MyDrive/Colab Notebooks/"